# Generative AI Project Template

A structured template for building robust generative AI applications, with modular organization and best practices built-in.

## ğŸŒŸ Features

- Modular project structure for scalability
- Pre-configured support for multiple LLM providers (Claude, GPT)
- Built-in prompt engineering utilities
- Rate limiting and token management
- Robust error handling
- Caching mechanism for API responses
- Example implementations and notebooks

## ğŸ“ Project Structure

```
generative_ai_project/
â”œâ”€â”€ config/                  # Configuration directory
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_config.yaml    # Model-specific configurations
â”‚   â”œâ”€â”€ prompt_templates.yaml # Prompt templates
â”‚   â””â”€â”€ logging_config.yaml  # Logging settings
â”‚
â”œâ”€â”€ src/                     # Source code
â”‚   â”œâ”€â”€ llm/                # LLM clients
â”‚   â”‚   â”œâ”€â”€ base.py         # Base LLM client
â”‚   â”‚   â”œâ”€â”€ claude_client.py # Anthropic Claude client
â”‚   â”‚   â”œâ”€â”€ gpt_client.py   # OpenAI GPT client
â”‚   â”‚   â””â”€â”€ utils.py        # Shared utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ prompt_engineering/ # Prompt engineering tools
â”‚   â”‚   â”œâ”€â”€ templates.py    # Template management
â”‚   â”‚   â”œâ”€â”€ few_shot.py    # Few-shot prompt utilities
â”‚   â”‚   â””â”€â”€ chain.py       # Prompt chaining logic
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/             # Utility functions
â”‚   â”‚   â”œâ”€â”€ rate_limiter.py # API rate limiting
â”‚   â”‚   â”œâ”€â”€ token_counter.py # Token counting
â”‚   â”‚   â”œâ”€â”€ cache.py       # Response caching
â”‚   â”‚   â””â”€â”€ logger.py      # Logging utilities
â”‚   â”‚
â”‚   â””â”€â”€ handlers/          # Error handling
â”‚       â””â”€â”€ error_handler.py
â”‚
â”œâ”€â”€ data/                   # Data directory
â”‚   â”œâ”€â”€ cache/             # Cache storage
â”‚   â”œâ”€â”€ prompts/           # Prompt storage
â”‚   â”œâ”€â”€ outputs/           # Output storage
â”‚   â””â”€â”€ embeddings/        # Embedding storage
â”‚
â”œâ”€â”€ examples/              # Example implementations
â”‚   â”œâ”€â”€ basic_completion.py
â”‚   â”œâ”€â”€ chat_session.py
â”‚   â””â”€â”€ chain_prompts.py
â”‚
â””â”€â”€ notebooks/            # Jupyter notebooks
    â”œâ”€â”€ prompt_testing.ipynb
    â”œâ”€â”€ response_analysis.ipynb
    â””â”€â”€ model_experimentation.ipynb
```

## ğŸš€ Getting Started

1. Clone the repository:
```bash
git clone https://github.com/yourusername/generative_ai_project.git
cd generative_ai_project
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Configure your environment:
   - Copy `config/model_config.yaml.example` to `config/model_config.yaml`
   - Add your API keys and configurations

4. Review the examples in `examples/` directory

5. Start with the notebooks in `notebooks/` for experimentation

## ğŸ“˜ Documentation

### Configuration

- `model_config.yaml`: Configure API keys and model parameters
- `prompt_templates.yaml`: Define reusable prompt templates
- `logging_config.yaml`: Configure logging behavior

### Key Components

1. **LLM Clients** (`src/llm/`)
   - Base client with common functionality
   - Specific implementations for different providers
   - Utility functions for token counting and rate limiting

2. **Prompt Engineering** (`src/prompt_engineering/`)
   - Template management system
   - Few-shot prompt utilities
   - Prompt chaining capabilities

3. **Utilities** (`src/utils/`)
   - Rate limiting for API calls
   - Token counting
   - Response caching
   - Logging

## ğŸ› ï¸ Development

### Best Practices

1. Keep configuration in YAML files
2. Implement proper error handling
3. Use rate limiting for APIs
4. Maintain separation between model clients
5. Cache results when appropriate
6. Document your code
7. Use notebooks for experimentation

### Tips

- Follow modular design principles
- Write tests for new components
- Use proper version control
- Keep documentation updated
- Monitor API usage and limits

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¤ Author

- **Brij Kishore Pandey**

## ğŸ“§ Contact

For any queries, reach out to:
- GitHub: [@honestsoul](https://github.com/honestsoul)
- Email: brij.pydata@gmail.com

---
â­ If you find this template useful, please consider giving it a star!
